{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6899fe85-cdfd-4c17-8be3-023cc53f212c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/c/Anaconda/python\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b9e93c12-8775-4350-9400-3dd0b1ca1581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.20.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff721ad-dc2f-443d-b486-88bba121ac92",
   "metadata": {},
   "source": [
    "# Deep Learning MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ad892d-e83e-4a2c-bf88-85ab69d6f835",
   "metadata": {},
   "source": [
    "# MNIST Handwritten Digit Classification Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fd1b4a-f34b-45d1-8c2f-4a628e61d13f",
   "metadata": {},
   "source": [
    "# The MINIST dataset is an acronym that stands for the Modified National Institute of Standards and Technolgy dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f184e5f-2849-4580-83c1-5f57b9db3080",
   "metadata": {},
   "source": [
    "# It is dataset of 60,000 small square 28 * 28 pixel grayscale images of hanwritten single digits between 0 t 9."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0ebbfd-f060-47c2-a8ce-9fb1e7f675e0",
   "metadata": {},
   "source": [
    "# The task is to classify a given images of a hanwritten digit into one of 10 classes representing integer value from 0 to 9, Inclusively"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a20a39-bf1f-40f8-b7c7-755b7a1655ec",
   "metadata": {},
   "source": [
    "# It is a widely used and deeply understood dataset and, for the most part is \"sloved\" Top-performing models are deep learning convolution neural networks that achieve a classification accuracy of above 99% with an error rate between 0.4% and 0.2% on the hold out test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c485f226-aedb-45d5-90dc-0a87d1ec413c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6e9391-5983-4d4f-af4e-9d8816e141ed",
   "metadata": {},
   "source": [
    "# TensorFlow is an open-sourced end-to-end plateform, a library for multiple machine learning tasks,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02391b93-9fcd-4e14-8785-47c9917267d4",
   "metadata": {},
   "source": [
    "# While keras is a high-level neural network library that runs on top of TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e12da87-a1a1-41f0-a4d4-968771963b62",
   "metadata": {},
   "source": [
    "# Both provide high-level neural network APIs used for easily building and training models, but Keras is more user-friendly because it's built-in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "99227616-5d1a-4e28-94c9-b8f087b330b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9abc3ce1-28e1-4d53-a680-bf9578068885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MNIST dataSet\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6968e5bf-bdc8-402f-81ab-671626c1eda2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8ebad219-743f-47c6-8087-350bc748bec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 28, 28)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "085bd3c4-d161-4d55-8142-90a913775d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the data\n",
    "x_train = x_train.reshape(x_train.shape[0], 28*28).astype('float32') /255\n",
    "x_test = x_test.reshape(x_test.shape[0], 28*28).astype('float32')/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e287fe48-602b-4283-b536-ba6c5f53598a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "042fe39b-4da7-4d03-ba9c-ee5f45bd8c21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e8bd89e6-91fd-45b0-a6cf-4f52c4e26dce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5a11949c-4993-45e2-aff8-a114a0a745e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert tabels to one-hot encoding\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "97262020-b808-4c58-b727-1fc506c330e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "61fd98ee-c7df-446b-9fbc-be59f21bc344",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "baadd94c-5939-484a-a8e0-1fc74f6fbd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the activation function\n",
    "def sigmoid(x):\n",
    "    return 1/ (1 + np.exp(-x))\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "def leaky_relu(x, alpha=0.01):\n",
    "    return np.maximum(alpha * x, x)\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x)\n",
    "    return exp_x / np.sum(exp_x, axis-1, keepdims = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e716db4a-9ddd-4a95-b98f-d51ea319044a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "model = keras.Sequential([\n",
    "    # extra\n",
    "      layers.Input(shape=(784,)),\n",
    "    # extra done \n",
    "    layers.Dense(128, input_shape = (28 * 28), activation = 'relu'),\n",
    "    layers.Dense(10, activation = 'softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "21b6f11e-70fb-4cb6-a22e-bd936f49895d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics= ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bb80aa-5632-48fe-8bea-80a4889fe9a4",
   "metadata": {},
   "source": [
    "# Adam is a popular algorithm in the field of deep learning because it achieves good results fast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89688402-60a6-448d-8cc8-0e0335227e19",
   "metadata": {},
   "source": [
    "# The categorical cross-entropy loss function will be optimized, suitable for multi-class classification, and we will monitor the classification accuracy metric, which is appropriate given we hav the same number of examples in each of the 10 classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213eb9f2-a574-4236-b598-451f0f95a0d2",
   "metadata": {},
   "source": [
    "# Minibatch size: when you are facing billions of data, it might result inefficient(as well as counterproductive)\n",
    "# feeding your NN with all of them. A good practice is feeding it with smaller samples of your data, called batches: by doing so, every time the algorithm train itself, it will train on a sample of the same size of the batch. The typical size is 32 or higher, however you need to keep in mind that, if the size is to big, the risk is an over generalized model which won't fit new data well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d2b23233-c080-45a7-a29c-1210fac3992e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.0394 - loss: 2.4286\n",
      "Test Loss:  2.428614854812622\n",
      "Test Accuracy:  0.039400000125169754\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print('Test Loss: ', test_loss)\n",
    "print('Test Accuracy: ', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "eb9f2601-9187-45db-bbb9-e6c3621cc391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9884 - loss: 0.0374\n",
      "Epoch 2/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9909 - loss: 0.0296\n",
      "Epoch 3/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 3ms/step - accuracy: 0.9928 - loss: 0.0239\n",
      "Epoch 4/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9944 - loss: 0.0186\n",
      "Epoch 5/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9944 - loss: 0.0178\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x21f8f145ca0>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train the model\n",
    "model.fit(x_train, y_train, epochs = 5, batch_size =32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5dfa1b1-a9c1-4e03-8e95-7c3002123ce0",
   "metadata": {},
   "source": [
    "# Epochs: it represents how many time you want your algorithm to train on your  whole dataset. Again, the number of epochs depends on the kind of data and task you facing. An idea could be imposing a condition such that epochs stop when the error is close to zero. Or, more easily, You can start with a relatively low number of epochs and then increase it progressively, tracking some evaluation metrics(like accuracy).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83617d77-ca3c-47f0-b849-586984c2f840",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1f014d-7aba-4021-ab8f-aaa90fa9d403",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3c3dc0-a02b-4bd8-bee5-d48fbb45d247",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6462a798-05da-4ffa-a972-fddd725cd3b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d80709-eea4-477c-b152-17eeeb059674",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab69f3d4-7eb2-4ef4-addc-395c2fca4e04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67e3f82-2630-4ac3-8be1-500ee9e75d33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9ab563-0550-48de-b292-652bfb847d26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e2951b-0b3a-4ff8-89cc-5fe8fcde1ac0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d735a69-3a20-48f9-8d7e-054c757e3dcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3f65b162-852f-4b62-8c5b-ad19e706d280",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git config --global user.email \"muzamilzubair987654@gmail.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b9a1a2c6-0385-41cf-b8e9-38142fdf2ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git config --global user.name \"Muzamil-Fatima\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd8a72fb-eef9-4754-8a13-e84b60198b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reinitialized existing Git repository in C:/Users/BEST LAPTOP/.git/\n"
     ]
    }
   ],
   "source": [
    "!git init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10fd952e-5c04-481f-afbc-22d8defa73a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error: remote origin already exists.\n"
     ]
    }
   ],
   "source": [
    "!git remote add origin https://github.com/Muzamil-Fatima/Deep-Learning.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e019211d-f883-4032-9207-50f4018d4f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "warning: in the working copy of 'DeepLearning.ipynb', LF will be replaced by CRLF the next time Git touches it\n"
     ]
    }
   ],
   "source": [
    "!git add DeepLearning.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7239fa6b-c507-42ed-87e8-3b235cb9109f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main 336c0c9] Upload from Jupyter\n",
      " 1 file changed, 651 insertions(+), 14 deletions(-)\n"
     ]
    }
   ],
   "source": [
    "!git commit -m \"Upload from Jupyter\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a482a998-dadd-49b8-8128-0e8088215e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "remote: This repository moved. Please use the new location:        \n",
      "remote:   https://github.com/Muzamil-Fatima/DataScience.git        \n",
      "To https://github.com/Muzamil-Fatima/NumPy.git\n",
      "   40f373c..336c0c9  main -> main\n"
     ]
    }
   ],
   "source": [
    "!git push origin main --force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce85c1a4-0b67-4c8f-95e5-6c2a09b9d364",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
