{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6899fe85-cdfd-4c17-8be3-023cc53f212c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/c/Anaconda/python\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b9e93c12-8775-4350-9400-3dd0b1ca1581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.20.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff721ad-dc2f-443d-b486-88bba121ac92",
   "metadata": {},
   "source": [
    "# Deep Learning MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ad892d-e83e-4a2c-bf88-85ab69d6f835",
   "metadata": {},
   "source": [
    "# MNIST Handwritten Digit Classification Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fd1b4a-f34b-45d1-8c2f-4a628e61d13f",
   "metadata": {},
   "source": [
    "# The MINIST dataset is an acronym that stands for the Modified National Institute of Standards and Technolgy dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f184e5f-2849-4580-83c1-5f57b9db3080",
   "metadata": {},
   "source": [
    "# It is dataset of 60,000 small square 28 * 28 pixel grayscale images of hanwritten single digits between 0 t 9."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0ebbfd-f060-47c2-a8ce-9fb1e7f675e0",
   "metadata": {},
   "source": [
    "# The task is to classify a given images of a hanwritten digit into one of 10 classes representing integer value from 0 to 9, Inclusively"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a20a39-bf1f-40f8-b7c7-755b7a1655ec",
   "metadata": {},
   "source": [
    "# It is a widely used and deeply understood dataset and, for the most part is \"sloved\" Top-performing models are deep learning convolution neural networks that achieve a classification accuracy of above 99% with an error rate between 0.4% and 0.2% on the hold out test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c485f226-aedb-45d5-90dc-0a87d1ec413c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72874b1b-8701-43fc-ade9-a23741acdafa",
   "metadata": {},
   "source": [
    "TensorFlow is an open-sourced end-to-end plateform, a library for multiple machine learning tasks,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d6d827-1b1b-4385-aaf5-d1fd809b4a32",
   "metadata": {},
   "source": [
    "While keras is a high-level neural network library that runs on top of TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abe148a-958e-40bf-b829-88cd5fd58fbb",
   "metadata": {},
   "source": [
    "Both provide high-level neural network APIs used for easily building and training models, but Keras is more user-friendly because it's built-in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "99227616-5d1a-4e28-94c9-b8f087b330b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9abc3ce1-28e1-4d53-a680-bf9578068885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MNIST dataSet\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6968e5bf-bdc8-402f-81ab-671626c1eda2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8ebad219-743f-47c6-8087-350bc748bec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 28, 28)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "085bd3c4-d161-4d55-8142-90a913775d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the data\n",
    "x_train = x_train.reshape(x_train.shape[0], 28*28).astype('float32') /255\n",
    "x_test = x_test.reshape(x_test.shape[0], 28*28).astype('float32')/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e287fe48-602b-4283-b536-ba6c5f53598a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "042fe39b-4da7-4d03-ba9c-ee5f45bd8c21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e8bd89e6-91fd-45b0-a6cf-4f52c4e26dce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5a11949c-4993-45e2-aff8-a114a0a745e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert tabels to one-hot encoding\n",
    "y_tain = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "97262020-b808-4c58-b727-1fc506c330e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, ..., 5, 6, 8], dtype=uint8)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "61fd98ee-c7df-446b-9fbc-be59f21bc344",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "baadd94c-5939-484a-a8e0-1fc74f6fbd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the activation function\n",
    "def sigmoid(x):\n",
    "    return 1/ (1 + np.exp(-x))\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "def leaky_relu(x, alpha=0.01):\n",
    "    return np.maximum(alpha * x, x)\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x)\n",
    "    return exp_x / np.sum(exp_x, axis-1, keepdims = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e716db4a-9ddd-4a95-b98f-d51ea319044a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot convert '784' to a shape.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[72], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Build the model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mSequential([\n\u001b[0;32m      3\u001b[0m     layers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;241m128\u001b[39m, input_shape \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m28\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m28\u001b[39m), activation \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m      4\u001b[0m     layers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;241m10\u001b[39m, activation \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m ])\n",
      "File \u001b[1;32mC:\\Anaconda\\Lib\\site-packages\\keras\\src\\models\\sequential.py:75\u001b[0m, in \u001b[0;36mSequential.__init__\u001b[1;34m(self, layers, trainable, name)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m layers:\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m layers:\n\u001b[1;32m---> 75\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd(layer, rebuild\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_rebuild()\n",
      "File \u001b[1;32mC:\\Anaconda\\Lib\\site-packages\\keras\\src\\models\\sequential.py:88\u001b[0m, in \u001b[0;36mSequential.add\u001b[1;34m(self, layer, rebuild)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layers:\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(layer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_input_shape_arg\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 88\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd(InputLayer(shape\u001b[38;5;241m=\u001b[39mlayer\u001b[38;5;241m.\u001b[39m_input_shape_arg))\n\u001b[0;32m     90\u001b[0m \u001b[38;5;66;03m# If we are passed a Keras tensor created by keras.Input(), we\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;66;03m# extract the input layer from its keras history and use that.\u001b[39;00m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(layer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_keras_history\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mC:\\Anaconda\\Lib\\site-packages\\keras\\src\\layers\\core\\input_layer.py:92\u001b[0m, in \u001b[0;36mInputLayer.__init__\u001b[1;34m(self, shape, batch_size, dtype, sparse, ragged, batch_shape, input_tensor, optional, name, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou must pass a `shape` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 92\u001b[0m         shape \u001b[38;5;241m=\u001b[39m backend\u001b[38;5;241m.\u001b[39mstandardize_shape(shape)\n\u001b[0;32m     93\u001b[0m         batch_shape \u001b[38;5;241m=\u001b[39m (batch_size,) \u001b[38;5;241m+\u001b[39m shape\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_shape \u001b[38;5;241m=\u001b[39m backend\u001b[38;5;241m.\u001b[39mstandardize_shape(batch_shape)\n",
      "File \u001b[1;32mC:\\Anaconda\\Lib\\site-packages\\keras\\src\\backend\\common\\variables.py:592\u001b[0m, in \u001b[0;36mstandardize_shape\u001b[1;34m(shape)\u001b[0m\n\u001b[0;32m    590\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUndefined shapes are not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(shape, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__iter__\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 592\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot convert \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to a shape.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    593\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mbackend() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensorflow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    594\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(shape, tf\u001b[38;5;241m.\u001b[39mTensorShape):\n\u001b[0;32m    595\u001b[0m         \u001b[38;5;66;03m# `tf.TensorShape` may contain `Dimension` objects.\u001b[39;00m\n\u001b[0;32m    596\u001b[0m         \u001b[38;5;66;03m# We need to convert the items in it to either int or `None`\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot convert '784' to a shape."
     ]
    }
   ],
   "source": [
    "# Build the model\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(128, input_shape = (28 * 28), activation = 'relu'),\n",
    "    layers.Dense(10, activation = 'softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "21b6f11e-70fb-4cb6-a22e-bd936f49895d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[73], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Compile the model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics= ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94f8b75-1ed7-42e1-8c00-ec14bbce2744",
   "metadata": {},
   "outputs": [],
   "source": [
    "Adam is a popular algorithm in the field of deep learning because it achieves good results fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "da1eb84b-2f06-45e8-bffc-5601fafb3094",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3913944200.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[76], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    The categorical cross-entropy loss function will be optimized, suitable for multi-class classification, and we will monitor the classification accuracy metric, which is appropriate given we hav the same number of examples in each of the 10 classes.\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "The categorical cross-entropy loss function will be optimized, suitable for multi-class classification, and we will monitor the classification accuracy metric, which is appropriate given we hav the same number of examples in each of the 10 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410b4fcc-b7f7-4dc1-b892-4df05228e7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Minibatch size: when you are facing billions of data, it might result inefficient(as well as counterproductive)\n",
    "feeding your NN with all of them. A good practice is feeding it with smaller samples of your data, called batches: by doing so, every time the algorithm train itself, it will train on a sample of the same size of the batch. The typical size is 32 or higher, however you need to keep in mind that, if the size is to big, the risk is an over generalized model which won't fit new data well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d2b23233-c080-45a7-a29c-1210fac3992e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[77], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Evaluate the model on the test set\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m test_loss, test_acc \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(x_test, y_test)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest Loss: \u001b[39m\u001b[38;5;124m'\u001b[39m, test_loss)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest Accuracy: \u001b[39m\u001b[38;5;124m'\u001b[39m, test_acc)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print('Test Loss: ', test_loss)\n",
    "print('Test Accuracy: ', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "eb9f2601-9187-45db-bbb9-e6c3621cc391",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[75], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Train the model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(x_train, y_train, epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m, batch_size \u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "#Train the model\n",
    "model.fit(x_train, y_train, epochs = 5, batch_size =32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e3a0b6-c444-4d94-8fc1-6f4f78d917f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Epochs: it represents how many time you want your algorithm to train on your  whole dataset. Again, the number of epochs depends on the kind of data and task you facing. An idea could be imposing a condition such that epochs stop when the error is close to zero. Or, more easily, You can start with a relatively low number of epochs and then increase it progressively, tracking some evaluation metrics(like accuracy).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "823e7c0f-4539-47f2-a519-743d59ad68e2",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3277417328.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[11], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    git init\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "git init\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62a2328-7e29-49c2-baa3-15525737f86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "git remote add origin https://github.com/Muzamil-Fatima/Deep-Learning.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e624bb-cce7-4d1f-a1b1-677f4633a2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "git commit -m \"first commit\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14237755-d258-4585-acf1-2b0dcd4a8415",
   "metadata": {},
   "outputs": [],
   "source": [
    "git push -u origin main\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f65b162-852f-4b62-8c5b-ad19e706d280",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git config --global user.email \"muzamilzubair987654@gmail.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9a1a2c6-0385-41cf-b8e9-38142fdf2ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git config --global user.name \"Muzamil-Fatima\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd8a72fb-eef9-4754-8a13-e84b60198b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reinitialized existing Git repository in C:/Users/BEST LAPTOP/.git/\n"
     ]
    }
   ],
   "source": [
    "!git init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "10fd952e-5c04-481f-afbc-22d8defa73a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error: remote origin already exists.\n"
     ]
    }
   ],
   "source": [
    "!git remote add origin https://github.com/Muzamil-Fatima/Deep-Learning.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e019211d-f883-4032-9207-50f4018d4f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add DeepLearning.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7239fa6b-c507-42ed-87e8-3b235cb9109f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On branch main\n",
      "Changes not staged for commit:\n",
      "  (use \"git add <file>...\" to update what will be committed)\n",
      "  (use \"git restore <file>...\" to discard changes in working directory)\n",
      "\tmodified:   DeepLearning.ipynb\n",
      "\n",
      "Untracked files:\n",
      "  (use \"git add <file>...\" to include in what will be committed)\n",
      "\t.anaconda/\n",
      "\t.bash_history\n",
      "\t.conda/\n",
      "\t.condarc\n",
      "\t.config/\n",
      "\t.console-ninja/\n",
      "\t.continuum/\n",
      "\t.cufflinks/\n",
      "\t.cursor/\n",
      "\t.docker/\n",
      "\t.dotnet/\n",
      "\t.gitconfig\n",
      "\t.ipynb_checkpoints/\n",
      "\t.ipython/\n",
      "\t.jupyter/\n",
      "\t.keras/\n",
      "\t.lesshst\n",
      "\t.matplotlib/\n",
      "\t.node_repl_history\n",
      "\t.nuget/\n",
      "\t.python_history\n",
      "\t.rest-client/\n",
      "\t.templateengine/\n",
      "\t.th-client/\n",
      "\t.virtual_documents/\n",
      "\t.vscode/\n",
      "\t3D Objects/\n",
      "\tAppData/\n",
      "\tContacts/\n",
      "\tDataScience/\n",
      "\tDesktop/\n",
      "\tDocuments/\n",
      "\tDownloads/\n",
      "\tFavorites/\n",
      "\tIntelGraphicsProfiles/\n",
      "\tLinks/\n",
      "\tLocal Sites/\n",
      "\tMatplotlibDoc.ipynb\n",
      "\tMicrosoftEdgeBackups/\n",
      "\tModelppt.ipynb\n",
      "\tMusic/\n",
      "\tMy GOAL.docx\n",
      "\tNCH Software Suite/\n",
      "\tNTUSER.DAT\n",
      "\tNTUSER.DAT{53b39e88-18c4-11ea-a811-000d3aa4692b}.TM.blf\n",
      "\tNTUSER.DAT{53b39e88-18c4-11ea-a811-000d3aa4692b}.TMContainer00000000000000000001.regtrans-ms\n",
      "\tNTUSER.DAT{53b39e88-18c4-11ea-a811-000d3aa4692b}.TMContainer00000000000000000002.regtrans-ms\n",
      "\tNumpyProject.ipynb\n",
      "\tOneDrive/\n",
      "\tPandasDoc.ipynb\n",
      "\tPictures/\n",
      "\tPlotlyDoc.ipynb\n",
      "\tSaved Games/\n",
      "\tSeabornDoc.ipynb\n",
      "\tSearches/\n",
      "\tTracing/\n",
      "\tUntitled1.ipynb\n",
      "\tVideos/\n",
      "\tgit\n",
      "\tmain\n",
      "\tntuser.dat.LOG1\n",
      "\tntuser.dat.LOG2\n",
      "\tntuser.ini\n",
      "\tsource/\n",
      "\ttodos-list/\n",
      "\ttodos-lists/\n",
      "\twsl\n",
      "\n",
      "no changes added to commit (use \"git add\" and/or \"git commit -a\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "warning: could not open directory 'Application Data/': Permission denied\n",
      "warning: could not open directory 'Cookies/': Permission denied\n",
      "warning: could not open directory 'Local Settings/': Permission denied\n",
      "warning: could not open directory 'My Documents/': Permission denied\n",
      "warning: could not open directory 'NetHood/': Permission denied\n",
      "warning: could not open directory 'PrintHood/': Permission denied\n",
      "warning: could not open directory 'Recent/': Permission denied\n",
      "warning: could not open directory 'SendTo/': Permission denied\n",
      "warning: could not open directory 'Start Menu/': Permission denied\n",
      "warning: could not open directory 'Templates/': Permission denied\n"
     ]
    }
   ],
   "source": [
    "!git commit -m \"Upload from Jupyter\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a482a998-dadd-49b8-8128-0e8088215e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Everything up-to-date\n"
     ]
    }
   ],
   "source": [
    "!git push origin main --force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce85c1a4-0b67-4c8f-95e5-6c2a09b9d364",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
